{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7cb0408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next big invention VgHVH3h&K3,'H3GKY3T\n",
      "fY3,GYglg(,YgG@53,G_3Y&H3hK'V_3gf3Y&H3V,G_53^K\n",
      "3l,G3sH3,sVH3YK3sH3fKrH3K%3Y&H3fY'HHY3,G_3Y&H3V,fY3YgrH3#3hK\n",
      "V_3f,^3Y&,Y3hH3GHH_3YK3Hx(VK'H3Y&Hr53,G_3h&,Y3hH3&,_3,3GHh3(,'Y3K%3fKrHY&gG@3Y&,Y3r,^sH3#3h,f3gG3Y&H3YHl&GKVK@gHf3fg_H3K%3Y&H3f(HlY'\n",
      "lYgKG3K%3,VV3Y&H3fYK'^3gG3&gf3*\n",
      "HfYgKG3Y&,Y3#3hK\n",
      "V_3&,CH3,G3Hx,r(VH3K%3Y&H3lK\n",
      "'fH3K%3Y&H3rKfY3('K%gVH3YH,r3rKY&H'3KG3Y&H3K((K'Y\n",
      "GgY^3YK3('K_\n",
      "lH3fY'\n",
      "lY\n",
      "'H3YK3sH3,sVH3YK3_K3h&,Y3Y&H^3,'H3,Vh,^f3%gG_gG@3K\n",
      "Y3h&HG3#3h,f3hK'DgG@3,G_3Y&H3(KK'3K%3Y&H3rKfY3(HK(VH3_gH_3YK3Y&H3s,lYH'g,3Y&,Y3hH3&,CH3YK3&,CH3,3VKY3K%3^H,'f3,@K53,G_3fK3Y&H^3@K3KG3YK3r^3@',GYH_3YK3fHCHG3K\n",
      "Y3h&,Y3#3&,CH3sHHG3fHGfH3K\n",
      "Y3K%3Y&H3f\n",
      "sTHlY3K%3Y&H3l,fH53,G_3Y&H^d'H3GKY3gGYH'HfYgG@3gG3K\n",
      "'3fKV,'k37&,Y3('HfHGY3#3h,GY3YK3&,CH3,3lKr(\n",
      "YH'k3#3&,_3,3@'H,Y3f\n",
      "sY3fg@Gg%gl,GY3flgHGlH3,G_3Y&H3r,gG3%'Kr3&gf3&,'_3YK3Y&H3('KsVHr3gG3Y&H3H,'Y&k3cHYdf3l,VV3Y&H3&\n",
      "r,G3fY'\n",
      "lY\n",
      "'H3K%3Y&H3f,rH3YgrH3,G_3Y&H3('KsVHr3Y&,Y3hH3&,CH3Y&H3hK'V_53Y&H^3,'H3fK3r\n",
      "l&3rK'H3Y&,G3Y&H3fgYH3K%3s\n",
      "gV_gG@f3&,CH3GK3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sys import stdout\n",
    "from model2 import network, vocab, char2idx, idx2char, input_dim, hidden_dim, num_layers\n",
    "\n",
    "prompt = \"The next big invention \"\n",
    "speech_len = 1000\n",
    "temperature = 0.55\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load model\n",
    "    speaker = network(input_dim, hidden_dim, num_layers)\n",
    "    speaker.load_state_dict(torch.load('./checkpoints/e2_1.2322.pth', map_location='cpu'))\n",
    "    speaker.batch_size = 1\n",
    "    speaker.reset()\n",
    "    speaker.eval()\n",
    "\n",
    "    output = prompt[-1]  # Start generation from last prompt letter\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Prime the model on the prompt (except last letter)\n",
    "        for letter in prompt[:-1]:\n",
    "            ip = torch.tensor([[char2idx[letter]]])\n",
    "            _ = speaker(ip)\n",
    "\n",
    "        stdout.write(prompt)\n",
    "\n",
    "        # Generate text\n",
    "        for _ in range(speech_len):\n",
    "            ip = torch.tensor([[char2idx[output]]])  # Feed in last predicted letter\n",
    "            logits = speaker(ip).squeeze()           # Output logits\n",
    "            probs = torch.softmax(logits / temperature, dim=0)  # Apply temperature\n",
    "            top_i = torch.multinomial(probs, 1).item()          # Sample from distribution\n",
    "            output = idx2char[top_i]\n",
    "            stdout.write(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc8e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d716e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
