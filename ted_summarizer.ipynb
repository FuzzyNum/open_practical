{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae40ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "import lxml.etree\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acfc6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('ted_en-20160408.xml'):\n",
    "    urllib.request.urlretrieve(\"https://github.com/oxford-cs-deepnlp-2017/practical-1/blob/master/ted_en-20160408.xml?raw=true\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "178ee1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2085"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = lxml.etree.parse('ted_en-20160408.xml')\n",
    "input_text = doc.xpath('//content/text()')\n",
    "label = doc.xpath('//head/keywords/text()')\n",
    "del doc\n",
    "len(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29ba68a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess sentences to exclude all characters except alphabets and numbers\n",
    "texts = [re.sub(r'\\([^)]*\\)', '',text) for text in input_text]\n",
    "texts = [re.sub('r([^a-zA-Z0-9\\s])',' ',text) for text in texts] #Included '.'\n",
    "texts = [re.sub('[^a-zA-Z0-9\\']',' ',text) for text in texts] #To replace '.' with ' '\n",
    "texts = [re.sub('[^a-zA-Z0-9 ]','',text) for text in texts]\n",
    "texts = [text.lower() for text in texts] #uppercase->lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0da3cd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of text greater than 500 words are: 2076\n"
     ]
    }
   ],
   "source": [
    "text_labels = zip(texts,label)\n",
    "texts = [text_label for text_label in text_labels if len(text_label[0]) > 500]\n",
    "print('number of text greater than 500 words are:',len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6af69028",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts,labels = zip(*texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9185631",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [words for text in texts for words in text.split()]\n",
    "words_count = Counter(words)\n",
    "words_most_common =[word for word,count in words_count.most_common(100)]\n",
    "words_least_common = [word for word,count in words_count.most_common() if count==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53c2c005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of Token: 1948385\n"
     ]
    }
   ],
   "source": [
    "to_remove = words_most_common + words_least_common\n",
    "words_to_remove = set(to_remove)\n",
    "tokens = [word for word in words if word not in words_to_remove] #will be used during T-SNE\n",
    "print('size of Token:',len(tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3cb397f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in text.split() if word not in words_to_remove]for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53d4a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels as ['ooo', 'Too', 'oEo', 'ooD', 'TEo', 'ToD', 'oED', 'TED']\n",
    "label_coded = ['ooo']*len(labels)\n",
    "\n",
    "for i,keyword in enumerate(labels):\n",
    "    key = keyword.split(', ')\n",
    "    label = list(label_coded[i])\n",
    "    if 'technology' in key:\n",
    "        label[0] = 'T'\n",
    "    if 'entertainment' in key:\n",
    "        label[1] = 'E'\n",
    "    if 'design' in key:\n",
    "        label[2] = 'D'\n",
    "    else:\n",
    "        pass\n",
    "    label_coded[i] =''.join(label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdb4ce1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ooo', 1130),\n",
       " ('Too', 389),\n",
       " ('oEo', 169),\n",
       " ('ooD', 158),\n",
       " ('ToD', 137),\n",
       " ('TEo', 36),\n",
       " ('TED', 33),\n",
       " ('oED', 24)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_labels=Counter(label_coded)\n",
    "label_count = [word_count for word_count in count_labels.most_common()]\n",
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98430588",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hotted = np.zeros(shape=(len(labels),8),dtype='int16')\n",
    "label_lookup = ['ooo', 'Too', 'oEo', 'ooD', 'TEo', 'ToD', 'oED', 'TED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97691a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "label_lookup = ['ooo', 'Too', 'oEo', 'ooD', 'TEo', 'ToD', 'oED', 'TED']\n",
    "for i,label in enumerate(label_coded):\n",
    "    one_hotted[i][label_lookup.index(label)] = 1\n",
    "print(one_hotted[:10])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f39a4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.append('<UNK>')\n",
    "tokens.append('<PAD>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3418e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29c7e056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 37328\n"
     ]
    }
   ],
   "source": [
    "print('size of vocabulary:',len(vocab))\n",
    "id2word = dict(enumerate(vocab))\n",
    "word2id = dict((val,key) for (key,val) in id2word.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10d39155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripping Text to fall within length of 500; incase if it is shorter then padd with '<UNK>'\n",
    "length = 500 #sentence length\n",
    "stripped_text = []#np.zeros((len(texts),length)\n",
    "for i,text in enumerate(texts):\n",
    "    inputs = []\n",
    "    if len(text) >= 500:\n",
    "        inputs.extend(text[:500])\n",
    "    else:\n",
    "        extra_length = 500-len(text)\n",
    "        extra = ['<PAD>']*extra_length\n",
    "        word_with_extra = text + extra\n",
    "        inputs.extend(word_with_extra)\n",
    "    stripped_text.append(inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84e332eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076\n"
     ]
    }
   ],
   "source": [
    "stripped_length = len(stripped_text)\n",
    "print(stripped_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44c9ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,code in enumerate(label_coded):\n",
    "    one_hotted[i][label_lookup.index(code)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3625e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "text_ids = []\n",
    "for text in stripped_text:\n",
    "    for word in text:\n",
    "        i = word2id[word]\n",
    "        inputs.append(i)\n",
    "    text_ids.append(inputs)\n",
    "    inputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e36da33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30582, 'even', 'even', 18598)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ids[0][100] , id2word[text_ids[0][100]], stripped_text[0][100], word2id['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d3fb7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(text_ids,one_hotted))\n",
    "tr_size = round(0.8*len(data))\n",
    "vl_size = round(0.1*len(data))\n",
    "te_size = tr_size + vl_size\n",
    "n_classes = one_hotted.shape[1]\n",
    "train_Xy , val_Xy , test_Xy = [],[],[]\n",
    "for i in np.arange(n_classes):\n",
    "    j = np.zeros(n_classes)\n",
    "    j[i] = 1\n",
    "    temp = [text_ohe for text_ohe in data if text_ohe[1][i]==j[i]]\n",
    "    temp_len = len(temp)\n",
    "    tr_split = round(temp_len*0.8)\n",
    "    val_split = round(temp_len*0.9)\n",
    "    train_Xy.extend(temp[:tr_split])\n",
    "    val_Xy.extend(temp[tr_split:val_split])\n",
    "    test_Xy.extend(temp[val_split:])\n",
    "random.shuffle(train_Xy)\n",
    "random.shuffle(val_Xy)\n",
    "random.shuffle(test_Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a20a4731",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = torch.from_numpy(np.array([i[0] for i in train_Xy]))\n",
    "train_labels = [i[1] for i in train_Xy]\n",
    "val_ids = torch.from_numpy(np.array([i[0] for i in val_Xy]))\n",
    "val_labels = [i[1] for i in val_Xy]\n",
    "test_ids = torch.from_numpy(np.array([i[0] for i in test_Xy]))\n",
    "test_labels = [i[1] for i in test_Xy]\n",
    "train_labels = torch.tensor([label.tolist().index(1) for label in train_labels], dtype=torch.long)\n",
    "val_labels = torch.tensor([label.tolist().index(1) for label in val_labels], dtype=torch.long)\n",
    "test_labels = torch.tensor([label.tolist().index(1) for label in test_labels], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9bc3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3dccee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
